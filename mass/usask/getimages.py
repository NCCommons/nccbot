# -*- coding: utf-8 -*-
"""
Usage:
python3 I:/ncc/nccbot/mass/usask/getimages.py
python3 core8/pwb.py mass/usask/getimages break

"""

import json
import os
import re
import sys
from pathlib import Path

import requests

# ---
from api_bots import printe
from bs4 import BeautifulSoup

# ---

main_dir = Path(__file__).parent
jsonfile = main_dir / "urls.json"
jsonimages = main_dir / "images.json"


def read_json_file(file_path):
    with open(file_path, "r") as file:
        data = json.load(file)
    return data


def extract_images_from_url(url):
    # Print the URL being processed
    printe.output(f"\t Processing URL: {url}")

    # Send a GET request to the URL and get the response
    response = requests.get(url)

    # Check if the response status code is 200 (OK)
    if response.status_code != 200:
        # Print an error message if the request failed
        printe.output(f"\t\t Failed to fetch content from {url}")

        # Return an empty dictionary
        return {}

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.text, "html.parser")

    # Create a dictionary to store the image URLs and their captions
    images_info = {}
    # Find all 'figure' tags in the HTML
    figure_tags = soup.find_all("figure")

    # Iterate over each 'figure' tag
    for n, figure_tag in enumerate(figure_tags):
        printe.output(f"\t<<purple>> >> figure {n+1}/{len(figure_tags)}")
        # Find the 'figcaption' tag within the 'figure' tag
        caption_tag = figure_tag.find("figcaption")

        # Get the text of the 'figcaption' tag and remove leading/trailing whitespaces
        caption = caption_tag.text.strip() if caption_tag else ""
        printe.output(f"\t\t <<yellow>> caption: <<default>> {caption}")
        # Find the 'img' tag within the 'figure' tag
        img_tag = figure_tag.find("img")

        # Check if 'img' tag exists
        if img_tag:
            # Get the value of the 'srcset' attribute of the 'img' tag
            img_srcset = img_tag.get("srcset", "").split(",")[0].split()[0] if img_tag.get("srcset", "") else ""
            img_src = img_tag.get("src", "")
            # printe.output(f'\t\t <<yellow>> img_srcset: <<default>> {img_srcset}')
            # printe.output(f'\t\t <<yellow>> img_src: <<default>> {img_src}')
            # Split the 'srcset' value by comma and get the first URL
            img_url = img_srcset
            if not img_srcset:
                img_url = img_src
                printe.output(f"\t\t <<red>> no srcset, use src")

            # Remove the dimension part from the URL using regex
            img_url = re.sub(r"-\d+x\d+(\.\w+)$", r"\1", img_url)
            printe.output(f"\t\t <<yellow>> img_url: <<default>> {img_url}")

            # Check if a valid image URL exists
            if img_url:
                # Add the image URL and its caption to the dictionary
                images_info[img_url] = caption

    # Return the dictionary containing the image URLs and captions
    return images_info


def main():
    # Read the JSON file
    data = read_json_file(jsonfile)

    # If 'test' is in the command line arguments, replace data with a test value
    if "test" in sys.argv:
        data = {
            "Online DICOM Image Viewer (ODIN): An Introduction and User Manual": {
                "url": "https://openpress.usask.ca/undergradimaging/chapter/online-dicom-image-viewer-odin-an-introduction-and-user-manual/",
                "images": {},
            }
        }

    # Initialize a counter
    n = 0

    # Iterate over each section and its corresponding data
    for section, section_data in data.items():
        # Increment the counter
        n += 1

        # Print the section being processed
        printe.output(f"<<yellow>> Processing section {n}/{len(data)}: {section}")

        # Get the URL from the section data
        url = section_data["url"]

        # Extract images from the URL
        images_info = extract_images_from_url(url)

        # If images are found, update the data with the extracted image information
        if images_info:
            data[section]["images"] = images_info
            if "break" in sys.argv:
                break

    # sort the data by if it has images
    data = dict(sorted(data.items(), key=lambda item: len(item[1]["images"]), reverse=True))

    # print how many has images and how many has no images
    printe.output(
        f"<<green>> Number of sections with images: {len([k for k, v in data.items() if len(v['images']) > 0])}"
    )

    printe.output(
        f"<<green>> Number of sections with no images: {len([k for k, v in data.items() if len(v['images']) == 0])}"
    )

    # print len of all images
    printe.output(f"<<green>> Number of images: {sum(len(v['images']) for k, v in data.items())}")

    if "test" not in sys.argv:
        # Save the updated data back to the JSON file
        with open(jsonimages, "w", encoding="utf-8") as file:
            json.dump(data, file, indent=2)


if __name__ == "__main__":
    main()
